{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With features as each team \n",
      "\n",
      "<b>FAIL: 22 Accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Configure the necessary Spark environment.  pyspark needs SPARK_HOME setup\n",
      "# so it knows how to start the Spark master and some local workers for you to use.\n",
      "import os\n",
      "# Fill this in with the path to the spark-0.9.1-bin-cdh4 folder you just downloaded\n",
      "# (e.g., /home/saasbook/spark-0.9.1-bin-cdh4)\n",
      "path_to_spark = \"/home/saasbook/datascience-sp14/spark-0.9.1-bin-cdh4\"\n",
      "os.environ['SPARK_HOME'] = path_to_spark\n",
      "\n",
      "# Set the python path so that we know where to find the pyspark files.\n",
      "import sys\n",
      "path_to_pyspark = os.path.join(path_to_spark, \"python\")\n",
      "sys.path.insert(0, path_to_pyspark)\n",
      "\n",
      "from pyspark import SparkContext\n",
      "# You can set the app name to whatever you want; this just affects what\n",
      "# will show up in the UI.\n",
      "app_name = \"dotaPredictions\"\n",
      "sc = SparkContext(\"local\", app_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "games = sc.textFile(\"/home/saasbook/datascience-sp14/dota/dota2prediction/trainingdata.txt\",10)\n",
      "heros = sc.textFile(\"/home/saasbook/datascience-sp14/dota/dota2prediction/datdota_hero_mapper2.csv\",10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def splitCSV(entry):\n",
      "    items = entry.split(\",\")\n",
      "    return hash(frozenset(items[0:5])), hash(frozenset(items[5:10])), int(items[10])\n",
      "\n",
      "gamesCSV = games.map(splitCSV)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gamesCSV.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[(1716998929, -1009904870, 2),\n",
        " (1501395420, 1345820571, 1),\n",
        " (897373725, -953110504, 2),\n",
        " (1972469980, -133900242, 1),\n",
        " (703272422, -460332540, 1),\n",
        " (580261451, -1981207330, 2),\n",
        " (-1898062952, -27757845, 1),\n",
        " (2136003203, -968506226, 1),\n",
        " (1094877347, 703601420, 1),\n",
        " (1229725317, -257042414, 2)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Separate training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training = sc.parallelize(gamesCSV.takeSample(False, 10000, 1))\n",
      "validation = sc.parallelize(gamesCSV.takeSample(False, 2500, 1))\n",
      "test = sc.parallelize(gamesCSV.takeSample(False, 2500, 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train the models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.recommendation import ALS\n",
      "\n",
      "pValidation = validation.map(lambda x: (x[0], x[1]))\n",
      "\n",
      "model4 = ALS.train(training, 4).predictAll(pValidation)\n",
      "model8 = ALS.train(training, 8).predictAll(pValidation)\n",
      "model12 = ALS.train(training, 12).predictAll(pValidation)\n",
      "model16 = ALS.train(training, 16).predictAll(pValidation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "def compute_accuracy(predicted, actual):\n",
      "    p_match = predicted.map(lambda x: (x[0]+x[1],x[2]))\n",
      "    a_match = actual.map(lambda x: (x[0]+x[1],x[2]))\n",
      "    both = p_match.join(a_match)\n",
      "    results = both.map(lambda x: 0 if (round(x[1][0]) == x[1][1]) else 1)\n",
      "    return results.reduce(lambda x, y: x + y)*1.0 / results.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print compute_accuracy(model4, validation)\n",
      "print compute_accuracy(model8, validation)\n",
      "print compute_accuracy(model12, validation)\n",
      "print compute_accuracy(model16, validation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.783618581907\n",
        "0.772616136919"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.787897310513"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.768948655257"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}